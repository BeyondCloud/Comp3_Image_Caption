{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\a1989\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This notebook uses TensorFlow version 1.11.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"\"\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import _pickle as cPickle\n",
    "\n",
    "print(\"This notebook uses TensorFlow version {}\".format(tf.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = cPickle.load(open('dataset/text/vocab.pkl', 'rb'))\n",
    "print('total {} vocabularies'.format(len(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_vocab_occurance(vocab, df):\n",
    "    voc_cnt = {v: 0 for v in vocab}\n",
    "    for img_id, row in df.iterrows():\n",
    "        for w in row['caption'].split(' '):\n",
    "            voc_cnt[w] += 1\n",
    "    return voc_cnt\n",
    "\n",
    "\n",
    "df_train = pd.read_csv(os.path.join('dataset', 'train.csv'))\n",
    "\n",
    "print('count vocabulary occurances...')\n",
    "voc_cnt = count_vocab_occurance(vocab, df_train)\n",
    "\n",
    "# remove words appear < 50 times\n",
    "thrhd = 50\n",
    "x = np.array(list(voc_cnt.values()))\n",
    "print('{} words appear >= 50 times'.format(np.sum(x[(-x).argsort()] >= thrhd)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_voc_mapping(voc_cnt, thrhd):\n",
    "    \"\"\"\n",
    "    enc_map: voc --encode--> id\n",
    "    dec_map: id --decode--> voc\n",
    "    \"\"\"\n",
    "\n",
    "    def add(enc_map, dec_map, voc):\n",
    "        enc_map[voc] = len(dec_map)\n",
    "        dec_map[len(dec_map)] = voc\n",
    "        return enc_map, dec_map\n",
    "\n",
    "    # add <ST>, <ED>, <RARE>\n",
    "    enc_map, dec_map = {}, {}\n",
    "    for voc in ['<ST>', '<ED>', '<RARE>']:\n",
    "        enc_map, dec_map = add(enc_map, dec_map, voc)\n",
    "    for voc, cnt in voc_cnt.items():\n",
    "        if cnt < thrhd:  # rare words => <RARE>\n",
    "            enc_map[voc] = enc_map['<RARE>']\n",
    "        else:\n",
    "            enc_map, dec_map = add(enc_map, dec_map, voc)\n",
    "    return enc_map, dec_map\n",
    "\n",
    "\n",
    "enc_map, dec_map = build_voc_mapping(voc_cnt, thrhd)\n",
    "# save enc/decoding map to disk\n",
    "cPickle.dump(enc_map, open('dataset/text/enc_map.pkl', 'wb'))\n",
    "cPickle.dump(dec_map, open('dataset/text/dec_map.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def caption_to_ids(enc_map, df):\n",
    "    img_ids, caps = [], []\n",
    "    for idx, row in df.iterrows():\n",
    "        icap = [enc_map[x] for x in row['caption'].split(' ')]\n",
    "        icap.insert(0, enc_map['<ST>'])\n",
    "        icap.append(enc_map['<ED>'])\n",
    "        img_ids.append(row['img_id'])\n",
    "        caps.append(icap)\n",
    "    return pd.DataFrame({\n",
    "              'img_id': img_ids,\n",
    "              'caption': caps\n",
    "            }).set_index(['img_id'])\n",
    "\n",
    "\n",
    "enc_map = cPickle.load(open('dataset/text/enc_map.pkl', 'rb'))\n",
    "print('[transform captions into sequences of IDs]...')\n",
    "df_proc = caption_to_ids(enc_map, df_train)\n",
    "df_proc.to_csv('dataset/text/train_enc_cap.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cap = pd.read_csv(\n",
    "    'dataset/text/train_enc_cap.csv')  # a dataframe - 'img_id', 'cpation'\n",
    "enc_map = cPickle.load(\n",
    "    open('dataset/text/enc_map.pkl', 'rb'))  # token => id\n",
    "dec_map = cPickle.load(\n",
    "    open('dataset/text/dec_map.pkl', 'rb'))  # id => token\n",
    "vocab_size = len(dec_map)\n",
    "\n",
    "\n",
    "def decode(dec_map, ids):\n",
    "    \"\"\"decode IDs back to origin caption string\"\"\"\n",
    "    return ' '.join([dec_map[x] for x in ids])\n",
    "\n",
    "\n",
    "print('decoding the encoded captions back...\\n')\n",
    "for idx, row in df_cap.iloc[:8].iterrows():\n",
    "    print('{}: {}'.format(idx, decode(dec_map, eval(row['caption']))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tfrecords(df_cap, img_df, filename, num_files=5):\n",
    "    ''' create tfrecords for dataset '''\n",
    "\n",
    "    def _float_feature(value):\n",
    "        return tf.train.Feature(\n",
    "            float_list=tf.train.FloatList(value=value))\n",
    "\n",
    "    def _int64_feature(value):\n",
    "        return tf.train.Feature(\n",
    "            int64_list=tf.train.Int64List(value=value))\n",
    "\n",
    "    num_records_per_file = img_df.shape[0] // num_files\n",
    "\n",
    "    total_count = 0\n",
    "\n",
    "    print(\"create training dataset....\")\n",
    "    for i in range(num_files):\n",
    "        # tfrecord writer: write record into files\n",
    "        count = 0\n",
    "        writer = tf.python_io.TFRecordWriter(\n",
    "            filename + '-' + str(i + 1) +'.tfrecords')\n",
    "        \n",
    "        # start point (inclusive)\n",
    "        st = i * num_records_per_file  \n",
    "        # end point (exclusive)\n",
    "        ed = (i + 1) * num_records_per_file if i != num_files - 1 else img_df.shape[0]  \n",
    "\n",
    "        for idx, row in img_df.iloc[st:ed].iterrows():\n",
    "        \n",
    "            # img representation in 256-d array format\n",
    "            img_representation = row['img']  \n",
    "\n",
    "            # each image has some captions describing it.\n",
    "            for _, inner_row in df_cap[df_cap['img_id'] == row['img_id']].iterrows():\n",
    "                # caption in different sequence length list format\n",
    "                caption = eval(inner_row['caption'])  \n",
    "\n",
    "                # construct 'example' object containing 'img', 'caption'\n",
    "                example = tf.train.Example(features=tf.train.Features(\n",
    "                    feature={\n",
    "                        'img': _float_feature(img_representation),\n",
    "                        'caption': _int64_feature(caption)\n",
    "                    }))\n",
    "\n",
    "                count += 1\n",
    "                writer.write(example.SerializeToString())\n",
    "        print(\"create {}-{}.tfrecords -- contains {} records\".format(\n",
    "                                    filename, str(i + 1), count))\n",
    "        total_count += count\n",
    "        writer.close()\n",
    "    print(\"Total records: {}\".format(total_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "training_filenames = glob.glob('dataset/tfrecords/train-*')\n",
    "\n",
    "# get the number of records in training files\n",
    "def get_num_records(files):\n",
    "    count = 0\n",
    "    for fn in files:\n",
    "        for record in tf.python_io.tf_record_iterator(fn):\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "num_train_records = get_num_records(training_filenames)\n",
    "print('Number of training records in all training file: {}'.format(\n",
    "    num_train_records))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_parser(record):\n",
    "    ''' parse record from .tfrecords file and create training record\n",
    "\n",
    "    :args \n",
    "      record - each record extracted from .tfrecords\n",
    "    :return\n",
    "      a dictionary contains {\n",
    "          'img': image array extracted from vgg16 (256-dim),\n",
    "          'input_seq': a list of word id\n",
    "                    which describes input caption sequence (Tensor),\n",
    "          'output_seq': a list of word id\n",
    "                    which describes output caption sequence (Tensor),\n",
    "          'mask': a list of one which describe\n",
    "                    the length of input caption sequence (Tensor)\n",
    "      }\n",
    "    '''\n",
    "\n",
    "    keys_to_features = {\n",
    "      \"img\": tf.FixedLenFeature([256], dtype=tf.float32),\n",
    "      \"caption\": tf.VarLenFeature(dtype=tf.int64)\n",
    "    }\n",
    "\n",
    "    # features contains - 'img', 'caption'\n",
    "    features = tf.parse_single_example(record, features=keys_to_features)\n",
    "\n",
    "    img = features['img']\n",
    "    caption = features['caption'].values\n",
    "    caption = tf.cast(caption, tf.int32)\n",
    "\n",
    "    # create input and output sequence for each training example\n",
    "    # e.g. caption :   [0 2 5 7 9 1]\n",
    "    #      input_seq:  [0 2 5 7 9]\n",
    "    #      output_seq: [2 5 7 9 1]\n",
    "    #      mask:       [1 1 1 1 1]\n",
    "    caption_len = tf.shape(caption)[0]\n",
    "    input_len = tf.expand_dims(tf.subtract(caption_len, 1), 0)\n",
    "\n",
    "    input_seq = tf.slice(caption, [0], input_len)\n",
    "    output_seq = tf.slice(caption, [1], input_len)\n",
    "    mask = tf.ones(input_len, dtype=tf.int32)\n",
    "\n",
    "    records = {\n",
    "      'img': img,\n",
    "      'input_seq': input_seq,\n",
    "      'output_seq': output_seq,\n",
    "      'mask': mask\n",
    "    }\n",
    "\n",
    "    return records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfrecord_iterator(filenames, batch_size, record_parser):\n",
    "    ''' create iterator to eat tfrecord dataset \n",
    "\n",
    "    :args\n",
    "        filenames     - a list of filenames (string)\n",
    "        batch_size    - batch size (positive int)\n",
    "        record_parser - a parser that read tfrecord\n",
    "                        and create example record (function)\n",
    "\n",
    "    :return \n",
    "        iterator      - an Iterator providing a way\n",
    "                        to extract elements from the created dataset.\n",
    "        output_types  - the output types of the created dataset.\n",
    "        output_shapes - the output shapes of the created dataset.\n",
    "    '''\n",
    "    dataset = tf.data.TFRecordDataset(filenames)\n",
    "    dataset = dataset.map(record_parser, num_parallel_calls=16)\n",
    "\n",
    "    # padded into equal length in each batch\n",
    "    dataset = dataset.padded_batch(\n",
    "      batch_size=batch_size,\n",
    "      padded_shapes={\n",
    "          'img': [None],\n",
    "          'input_seq': [None],\n",
    "          'output_seq': [None],\n",
    "          'mask': [None]\n",
    "      },\n",
    "      padding_values={\n",
    "          'img': 1.0,       # needless, for completeness\n",
    "          'input_seq': 1,   # padding input sequence in this batch\n",
    "          'output_seq': 1,  # padding output sequence in this batch\n",
    "          'mask': 0         # padding 0 means no words in this position\n",
    "      })  \n",
    "\n",
    "    dataset = dataset.repeat()             # repeat dataset infinitely\n",
    "    dataset = dataset.shuffle(3*batch_size)  # shuffle the dataset\n",
    "\n",
    "    iterator = dataset.make_initializable_iterator()\n",
    "    output_types = dataset.output_types\n",
    "    output_shapes = dataset.output_shapes\n",
    "\n",
    "    return iterator, output_types, output_shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_seq_embeddings(input_seq, vocab_size, word_embedding_size):\n",
    "    with tf.variable_scope('seq_embedding'), tf.device(\"/cpu:0\"):\n",
    "        embedding_matrix = tf.get_variable(\n",
    "            name='embedding_matrix',\n",
    "            shape=[vocab_size, word_embedding_size],\n",
    "            initializer=tf.random_uniform_initializer(minval=-1, maxval=1))\n",
    "        # [batch_size, padded_length, embedding_size]\n",
    "        seq_embeddings = tf.nn.embedding_lookup(embedding_matrix, input_seq)\n",
    "    return seq_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageCaptionModel(object):\n",
    "    ''' simple image caption model '''\n",
    "\n",
    "    def __init__(self, hparams):\n",
    "        self.hps = hparams\n",
    "\n",
    "    def _build_inputs(self):\n",
    "        \"\"\" construct the inputs for model \"\"\"\n",
    "        self.filenames = tf.placeholder(tf.string,\n",
    "                                        shape=[None], name='filenames')\n",
    "        self.training_iterator, types, shapes = tfrecord_iterator(\n",
    "          self.filenames, self.hps.batch_size, training_parser)\n",
    "\n",
    "        self.handle = tf.placeholder(tf.string, shape=[], name='handle')\n",
    "        iterator = tf.data.Iterator.from_string_handle(self.handle,\n",
    "                                                       types, shapes)\n",
    "        records = iterator.get_next()\n",
    "\n",
    "        image_embed = records['img']\n",
    "        image_embed.set_shape([None, self.hps.image_embedding_size])\n",
    "        input_seq = records['input_seq']\n",
    "        target_seq = records['output_seq']\n",
    "        input_mask = records['mask']\n",
    "        \n",
    "        self.image_embed = image_embed # (batch_size, img_dim)\n",
    "        self.input_seq = input_seq # (batch_size, seqlen)\n",
    "        self.target_seq = target_seq # (batch_size, seqlen)\n",
    "        self.input_mask = input_mask # (batch_size, seqlen)\n",
    "            \n",
    "        # convert sequence of index to sequence of embedding\n",
    "        with tf.variable_scope('seq_embedding'), tf.device('/cpu:0'):\n",
    "            self.embedding_matrix = tf.get_variable(\n",
    "                    name='embedding_matrix',\n",
    "                    shape=[self.hps.vocab_size,\n",
    "                           self.hps.word_embedding_size],\n",
    "                    initializer=tf.random_uniform_initializer(\n",
    "                        minval=-1, maxval=1))\n",
    "            # [batch_size, seqlen, embedding_size]\n",
    "            seq_embeddings = tf.nn.embedding_lookup(\n",
    "                self.embedding_matrix, self.input_seq)\n",
    "        \n",
    "    def _build_model(self):\n",
    "        \"\"\" Build your image caption model \"\"\"\n",
    "        pass\n",
    "        \n",
    "    def build(self):\n",
    "        \"\"\" call this function to build the inputs and model \"\"\"\n",
    "        self._build_inputs()\n",
    "        self._build_model()\n",
    "        \n",
    "    def train(self, sess, training_filenames, num_train_records):\n",
    "        \"\"\" write a training function for your model \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def predict(self, sess, img_vec, dec_map):\n",
    "        \"\"\" generate the caption given an image \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hparams():\n",
    "    hparams = tf.contrib.training.HParams(\n",
    "      vocab_size=vocab_size,\n",
    "      batch_size=64,\n",
    "      rnn_units=100,\n",
    "      image_embedding_size=256,\n",
    "      word_embedding_size=256,\n",
    "      drop_keep_prob=0.7,\n",
    "      lr=1e-3,\n",
    "      training_epochs=1,\n",
    "      max_caption_len=15,\n",
    "      ckpt_dir='model_ckpt/')\n",
    "    return hparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get hperparameters\n",
    "hparams = get_hparams()\n",
    "# create model\n",
    "model = ImageCaptionModel(hparams)\n",
    "model.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start training\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)\n",
    "model.train(sess, training_filenames, num_train_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "model = ImageCaptionModel(hparams)\n",
    "model.build()\n",
    "\n",
    "# sample one image in training data and generate caption\n",
    "testimg = img_train_df.iloc[9]['img']\n",
    "testimg = np.expand_dims(testimg, axis=0)\n",
    "\n",
    "with tf.Session(config=config) as sess:\n",
    "    saver = tf.train.Saver()\n",
    "    # restore variables from disk.\n",
    "    ckpt = tf.train.get_checkpoint_state(hparams.ckpt_dir)\n",
    "    if ckpt and ckpt.model_checkpoint_path:\n",
    "        saver.restore(sess,\n",
    "                      tf.train.latest_checkpoint(hparams.ckpt_dir))\n",
    "        caption = model.predict(sess, testimg, dec_map)\n",
    "        print(caption)\n",
    "    else:\n",
    "        print(\"No checkpoint found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image, display\n",
    "from pretrained.cnn import PretrainedCNN\n",
    "import imageio\n",
    "import skimage.transform\n",
    "import numpy as np\n",
    "import scipy\n",
    "\n",
    "def demo(img_path, cnn_mdl, U, dec_map, hparams, max_len=15):\n",
    "    \"\"\"\n",
    "    displays the caption generated for the image\n",
    "    -------------------------------\n",
    "    img_path: image to be captioned\n",
    "    cnn_mdl: path of the image feature extractor\n",
    "    U: transform matrix to perform PCA\n",
    "    dec_map: mapping of vocabulary ID => token string\n",
    "    hparams: hyperparams for model\n",
    "    \"\"\"\n",
    "\n",
    "    def process_image(img, crop=True, submean=True):\n",
    "        \"\"\"\n",
    "        implements the image preprocess required by VGG-16\n",
    "        -------------------------------\n",
    "        resize image to 224 x 224\n",
    "        crop: do center-crop [skipped by default]\n",
    "        submean: substracts mean image of ImageNet [skipped by default]\n",
    "        \"\"\"\n",
    "        MEAN = np.array([103.939, 116.779, 123.68]).astype(np.float32) # BGR\n",
    "        # center crop\n",
    "        short_edge = min(img.shape[:2])\n",
    "        yy = int((img.shape[0] - short_edge) / 2)\n",
    "        xx = int((img.shape[1] - short_edge) / 2)\n",
    "        crop_img = img[yy: yy + short_edge, xx: xx + short_edge]\n",
    "        img = skimage.transform.resize(crop_img, [224, 224, 3], mode=\"constant\")\n",
    "        img = img.reshape((224,224,1)) if len(img.shape) < 3 else img\n",
    "        \n",
    "        if img.shape[2] < 3:\n",
    "            print('dimension insufficient')\n",
    "            img = img.reshape((224*224,\n",
    "                               img.shape[2])).T.reshape((img.shape[2],\n",
    "                                                                 224*224))\n",
    "            for i in range(img.shape[0], 3):\n",
    "                img = np.vstack([img, img[0,:]])\n",
    "            img = img.reshape((3,224*224)).T.reshape((224,224,3))\n",
    "        img = img.astype(np.float32)\n",
    "        img = img[:,:,::-1]\n",
    "        # RGB => BGR\n",
    "        for i in range(3):\n",
    "            img[:,:,i] -= MEAN[i]\n",
    "        return img.reshape((224,224,3))\n",
    "\n",
    "    display(Image(img_path))\n",
    "    img = imageio.imread(img_path)\n",
    "    \n",
    "    # load pretrained cnn model\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    with tf.Session(config=config) as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        img_feature = np.dot(\n",
    "            cnn_mdl.get_output(sess, [process_image(img)])[0].reshape((-1)), U)\n",
    "        \n",
    "    # reset graph for image caption model\n",
    "    tf.reset_default_graph()  \n",
    "    model = ImageCaptionModel(hparams)\n",
    "    model.build()\n",
    "    with tf.Session(config=config) as sess:\n",
    "        saver = tf.train.Saver()\n",
    "        # restore variables from disk.\n",
    "        ckpt = tf.train.get_checkpoint_state(hparams.ckpt_dir)\n",
    "        if ckpt and ckpt.model_checkpoint_path:\n",
    "            saver.restore(sess, tf.train.latest_checkpoint(hparams.ckpt_dir))\n",
    "            caption = model.predict(sess, img_feature, dec_map)\n",
    "            print(' '.join(caption))\n",
    "        else:\n",
    "            print(\"No checkpoint found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()  # reset graph for cnn model\n",
    "U = cPickle.load(open('dataset/U.pkl', 'rb'))  # PCA transforming matrix\n",
    "vgg = PretrainedCNN('pretrained/vgg16_mat.pkl')\n",
    "demo('demo/example1.jpg', vgg, U, dec_map, hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_captions(model, dec_map, img_test, max_len=15):\n",
    "    img_ids, caps = [], []\n",
    "  \n",
    "    with tf.Session() as sess:\n",
    "        saver = tf.train.Saver()\n",
    "        # restore variables from disk.\n",
    "        ckpt = tf.train.get_checkpoint_state(hparams.ckpt_dir)\n",
    "        if ckpt and ckpt.model_checkpoint_path:\n",
    "            saver.restore(sess,\n",
    "                          tf.train.latest_checkpoint(hparams.ckpt_dir))\n",
    "            for img_id, img in img_test.items():\n",
    "                img_ids.append(img_id)\n",
    "                caps.append(model.predict(sess, img, dec_map))\n",
    "        else:\n",
    "            print(\"No checkpoint found.\")\n",
    "    \n",
    "    return pd.DataFrame({\n",
    "              'img_id': img_ids,\n",
    "              'caption': caps\n",
    "            }).set_index(['img_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load test image  size=20548\n",
    "img_test = cPickle.load(open('dataset/test_img256.pkl', 'rb'))\n",
    "\n",
    "# create model\n",
    "tf.reset_default_graph()\n",
    "model = ImageCaptionModel(hparams)\n",
    "model.build()\n",
    "\n",
    "# generate caption to csv file\n",
    "df_predict = generate_captions(model, dec_map, img_test)\n",
    "df_predict.to_csv('generated/demo.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
